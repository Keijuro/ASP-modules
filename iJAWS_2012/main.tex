% This is "aamas2012 .tex" August 2012 
% This file should be compiled with "aamas2012 .cls" 
% This example file demonstrates the use of the 'aamas2012 .cls'
% LaTeX2e document class file. It is for those submitting
% articles to AAMAS 2012  conference. This file is based on
% the sig-alternate.tex example file.
% The 'sig-alternate.cls' file of ACM will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
% than the original style ACM style.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls ) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with AAMAS data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'aamas2012 .cls' you don't have control
% from within the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the IFAAMAS Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% These information will be overwritten by fixed AAMAS 2012  information
% in the style files - it is NOT as you are used with ACM style files.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%

\newtheorem{note}{Note}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

% This is the document class for full camera ready papers and extended abstracts repsectively 

\documentclass{aamas2012}

\usepackage{graphicx}
\usepackage{color}
\usepackage{algorithm,algorithmic}
\usepackage{multirow}
\usepackage{float}

\usepackage{module}

\graphicspath{{pictures/}}

% if you are using PDF LaTex and you cannot find a way for producing
% letter, the following explicit settings may help
 
\pdfpagewidth=8.5truein
\pdfpageheight=11truein

\begin{document}

	\begin{figure*}
		\includegraphics{page1.pdf}
	\end{figure*}
	
	\clearpage

\section{Introduction and motivation}

	In recent years, research about reasoning of multi-agent systems is very active.
	Some research works are concerned about knowledge representation like \cite{DBLP:conf/birthday/BaralG11},
	where authors discuss about representing actions in dynamic environment.
	There are also interesting works on meta-knowledge like \cite{DBLP:conf/atal/BaralGSP10}, 
	where agents have knowledge about other knowledge.
	In \cite{DBLP:conf/ijcai/SakamaSP11}, using this kind of meta-knowledge allows agents to lie to manipulate their fellows.
	Other works proposed agent knowledge representation like \cite{DBLP:conf/icmas/RaoG95}, 
	where agent knowledge is divide into beliefs, decisions and intentions (BDI).
	In \cite{DBLP:journals/amai/KowalskiS99} authors are concerned about representing agent reasoning via logic programming 
	and proposed an agent cycle based on three phases: observe, think, act.
	Regarding reasoning, there are works like \cite{DBLP:conf/datalog/Costantini10}, \cite{DBLP:conf/lpnmr/Costantini09}
	and \cite{DBLP:conf/aaaiss/BaralAD06} where authors are interested in modularity.
	In this work, our interest is about representing agent reasoning in dynamic environment.
	
	Here, we divide the knowledge of an agent in two part:
	\begin{itemize}
		\item $K$: non-revisable knowledge which consists of:
		\begin{itemize}
			\item $C$: the common theory
			\item $O$: the current observations
		\end{itemize}
		\item $B$: revisable knowledge which consists of:
		\begin{itemize}
			\item $M$: the past observations 
		\end{itemize}
	\end{itemize}
		
	In such environment, an agent has to adapt to the evolution of the world to achieve his goals.
	Agent's knowledge has to be updated regarding world changes by adding new observations and revising old ones.
	
	Let $C_{T}, O_{T}, M_{T}$ be $C$, $O$ and $M$ at time step T and $\circ$ a belief revision operator.
	\begin{itemize}
		\item $C_{0} = C$, $O_{0} = \emptyset$, $M_{0} = \emptyset$
		\item $C_{T+1} = C_{T}$, $O_{T} = obs(T)$, $M_{T+1} = M_{T} \diamond O_{T}$
	\end{itemize}
	
	$T$ is the common background knowledge of all agent of a multi-agent system, it is considered as certain and not revisable.
	A common theory can also be limited to a group of agent which does not contain all the system.
	Observations are informations retrieved from the environment, it represents the current state of the world.
	If we consider that sensors are perfect then a current observation is not revisable.
	At time step 0, the knowledge of an agent is his common theory.
	This knowledge is extended by adding observations and its consequences at each new time step.

	$B$ is agent's beliefs, it represents informations assumed to be true by the agent and which are revisable.
	In a dynamic environment, if current observations are certain, it is not the case of past ones.
	Here we assumes that past observations are correct until new ones prove the contrary.
	At time step $T+1$, agent past observations $M_{T+1}$ is the knowledge base resulting from updating $M_{T}$ by $O_{T+1}$ using the knowledge update operator $\diamond$.

	For example, let's suppose that an agent $a$ sees another agent $b$ at position $p$ at time $T$.
	Now, at $T+1$, $a$ sees neither the position $p$ nor the agent $b$, then $a$ can assume that $b$ is always at position $p$ at $T+1$.
	When $a$ sees again the position $p$ or the agent $b$, he will revise $M$ if needed.
	
	To make our work more understandable we will follow an intuitive example along our propositions: a survival game which represent a MAS in a dynamic environment.
	In this game there are three groups of agents: wolves, rabbits and flowers.
	Each kind of agent have specific goals and behaviours.
	To be simple, wolves eat rabbits and rabbits eat flowers.
	
	Wolves have only one goal: to feed.
	To reach this goal they have to catch and eat rabbits.
	A wolf can be in two situations: a prey is in sight or not.
	If there is no rabbit in the sight range of a wolf, the predator has to explore his environment to find one.
	When a prey is spotted, a wolf will try to perform a sneaky approach if he is not spotted himself, otherwise the predator will rush on his target.
	To summarise, a wolf have three behaviours: exploration, approach and attack.
	
	Rabbits have two goals : to feed and not to be eaten.
	On a first hand, a rabbit has to eat flowers and on an other hand it has to escape from wolf.
	Like a wolf, if no prey is in sight, a rabbit needs to explore its environment, but he can find preys and predators.
	The exploration behaviour of a rabbit is more complex than the one of a wolf.
	When a rabbit moves it could choose the position with the best sight range to spotted its predators.
	If a wolf is spotted, surviving is more important than feeding, therefore a rabbit has to hide or run away.
	To sum up, a rabbit has four behaviours: explore, feed, hide and run away.
	In this paper, examples and experiments focus on rabbit agent knowledge representation and reasoning.
	
	Flowers are passive agents: the environment has no effect on their behaviour.
	The only goal of a flower is to reproduce.
	A flower produces regularly a new one after a certain amount of time.
	Knowledge of a flower does not change regarding time.
	Flowers are independent, it does not care about other agents.

	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true,scale=3.0]{food_chain.png}
		\caption
		{
			\label{food_chain}
			A MAS in a dynamic environment where an agent is a wolf, a rabbit or a flower:
			Wolves eat rabbits and rabbits eat flowers.
		}
	\end{figure}
	
	The methods introduced by this work is based on modular knowledge representation.
	Agent knowledge is divided into different modules which are combined for reasoning.
	This representation allows agents to perform meta-reasoning by using knowledge on modules combinations.
	One interest is that an agent can adapt his reasoning and his behaviour to environment changes.

\section{Answer set programming}

	Answer set programming which is a specification and an implementation language, is very suitable to represent agent knowledge and reasoning.
	ASP is a form of declarative programming based on the stable model semantics of logic programming.
	In this section, we briefly introduce ASP paradigm and semantics.

	We recapitulate the basic elements of ASP in the following.
	An answer set program is a finite set of rules of the form
	$$a_{0}\ \leftarrow\ a_{1},\ \ldots,\ a_{m},\ not\ a_{m+1},\ \ldots,\ not\ a_{n}\ (1)$$
	where $n \ge m \ge 0$, $a_{0}$ is a propositional atom or $\bot$, all
	$a_{1}, \ldots ,a_{n}$ are propositional atoms and the symbol "$not$" denotes default negation.
	If $a_{0} = \bot$, then Rule $(1)$ is a constraint (in which case $a_{0}$ is usually omitted).
	The intuitive reading of a rule of form $(1)$ is that whenever $a_{1}, \ldots, a_{m}$
	are known to be true and there is no evidence for any of the default negated atoms $a_{m+1}, \ldots, a_{n}$ to be true, then $a_{0}$ has to be true as well.
	Note that $\bot$ can never become true.

	In the ASP paradigm, the search of solution consist to compute answer sets of ASP program.
	An answer set for a program is defined following Gelfond and Lifschitz \cite{DBLP:conf/iclp/GelfondL88}.
	An interpretation $I$ is a finite set of propositional atoms.
	An atom a is true under $I$ if a $\in I$, and false otherwise.
	A rule $r$ of form (1) is true under $I$ if $\{a1,\ \dots,\ a_{m}\} \cup I$ and $\{a_{m+1},\ \ldots,\ a_{n}\} \cup I = \emptyset$ implies $a_{0} \in\ I$.
	Interpretation $I$ is a model of a program $P$ if each rule $r \in P$ is true under $I$.
	Finally, $I$ is an answer set of $P$ if $I$ is a subset-minimal model of $P^{I}$,
	where $P^{I}$ is defined as the program that results from $P$ by deleting all rules that contain a default negated atom from $I$, 
	and deleting all default negated atoms from the remaining rules.
	Programs can yield no answer set, one answer set, or many answer sets.
	To compute answer set of an ASP program, we run an ASP solver.
	
	\begin{example}
		\label{ASP_example}
		An ASP program composed of one fact, three rules and one constraint :\newline
		\newline
		rain.\newline
		stay $\leftarrow$ not go\_out.\newline
		go\_out $\leftarrow$ not stay.\newline
		wet $\leftarrow$ rain, go\_out, not umbrella.\newline
		$\leftarrow$ wet.
	\end{example}
	
	In example \ref{ASP_example}, the set of facts \{\emph{rain}, \emph{stay}\} is an answer set of the ASP program, but \{\emph{rain}, \emph{go\_out}, \emph{wet}\} is not, 
	because it contains \emph{wet} which is not consistent with the constraint \emph{$\leftarrow$ wet}.
	
	Many research work use ASP to represent knowledge and reasoning such as \cite{DBLP:conf/atal/BaralGSP10} or \cite{DBLP:conf/clima/NieuwenborghVHV06}.
	In the first one, the authors use it to represent agent knowledge about other agents knowledge.
	In the second one, they focus on the flexibility of reasoning by introducing soft constraints: constraints which can be violate in some case.
	Other works like \cite{DBLP:conf/datalog/Costantini10} design ASP methods to improve some specific property of agent reasoning.
	In this work the author focuses on reactivity by using ASP modules where constraints are used as actions trigger.
	In \cite{DBLP:conf/lpnmr/Costantini09}, the author discuss about integration of ASP modules into agent reasoning.
	There are also works like \cite{DBLP:conf/aaaiss/BaralAD06} about modular ASP and 
	\cite{DBLP:conf/birthday/FaberW11} where they discuss about possible extensions of the paradigm.

\section{ASP modules}

	An ASP module is an ASP program which have a specific form and a specific use.
	The first advantage of these modules is their simplicity: a module is a little program which represents a specific knowledge.
	We can have a module which contains observations about surroundings,
	an other one to define what is a prey and a module dedicated to path computing.
	By combining these three modules, an agent can compute all paths to surrounding preys.
	To design agent knowledge we respect a module typology to represent background knowledge, observations and meta-knowledge.

\subsection{Typology}

	Following previous distinction of agent knowledge we define a typology to represent common theory $C$ and observations memory $M$,
	respectively by \emph{theory modules} and \emph{observations modules}.
	To represent knowledge about module combinations we define \emph{meta-knowledge modules}.
	There are is two kind of such modules, the ones who produce or use knowledge to make decision and the ones who represent a unique module combination.
	In the following figures, we represent \emph{observation modules} by dot rectangles and  \emph{theory} one by plain rectangles.
	To distinguish \emph{meta-knowledge modules} which make decisions, from the ones who represent a combination, we represent them respectively by plain and dot circles.
	Modules represented by dot form only contain facts and plain ones can contain rules and constraints.

	\begin{definition}[Theory module]
		A theory module is a set of rules which represent knowledge about a specific domain.
		The set of all theory modules of an agent represent his background knowledge.
		The purpose of these modules is to organise knowledge representation and produce
		new knowledge by combining it with others modules.
	\end{definition}
	
	\begin{example}
		\label{theory_example}
		
		A theory module of a rabbit about movement and one about actions where A is an agent, P a position and S a number of time step:\newline
		\begin{module}{Move}{8cm}
			reachable(A,P,$\frac{D}{N}$) $\leftarrow$ distance(A,P,D), nb\_action(A,N).\newline
			reachable(A,P) $\leftarrow$ reachable(A,P,0).
		\end{module}
		
		\begin{module}{Action}{8cm}
			nb\_action(me,3).\newline
			nb\_action(wolf,4).\newline
			0\{move(me,P)\}1 $\leftarrow$ reachable(me,P).\newline
			$\leftarrow$ move(A,P1), move(A,P2), P1 != P2.\newline
			$\leftarrow$ move(me,P), reachable(wolf,P).
		\end{module}
		
	\end{example}
	
	Example \ref{theory_example} shows two theory modules, the first one represents knowledge about movement possibilities
	and the second one represents knowledge about actions.
	To simplify the first module of this example, \emph{distance/3} is supposed given and 
	his third argument is the distance between an agent $A$ and a position $P$.
	Predicate \emph{nb\_action/2} specifies the number of actions that an agent can perform in one time step.
	The first module can be used by a rabbit to compute his movement possibility and the ones of his predators.
	The second module defines the movement action, it specifies that only one move can be performed and it cannot be in the movement range of a wolf.
	By combining these two modules with observations about wolves, a rabbit will produce all possible safe movements he can perform.

	\begin{definition}[Observations module]
		An observations module is a set of facts which represents related observations.
		The content of such module is dynamic: it changes regarding time.
		The set of all observations modules of an agent represents his memory M: current and past observations.
		The purpose of these modules is to organise observations to facilitate their use and update.
	\end{definition}
	
	\begin{example}
		An observations module of a rabbit about wolves positions and one about himself :\newline
		\begin{module}{Wolf}{6cm}
			position(wolf,coord(0,3)).\newline
			position(wolf,coord(2,5)).\newline
			position(wolf,coord(2,6)).
		\end{module}
		
		\begin{module}{Myself}{6cm}
			me("Bugs Bunny").\newline
			position(me,coord(0,0)).\newline
			type(me,rabbit).
		\end{module}
	\end{example}

	Using this two kind of modules, we can represent agent common theory and observations.
	Regarding how we combined these modules, we can produce different kinds of knowledge.
	The figure \ref{module_combination} represents the knowledge of a rabbit by a set of observations modules and theory modules.
	In this example, by combining modules \emph{Myself}, \emph{Field} and \emph{Move} a rabbit will produce all his possibility of movements.
	The combination \{\emph{Rabbit}, \emph{Field}, \emph{Move}\} will produce all possible movements that other rabbits can perform. 
	By combining \{\emph{Myself}, \emph{Field}, \emph{Flower}, \emph{Move}, \emph{Eat}, \emph{Action}\} 
	a rabbit will produce all possible actions he can perform now and their influence on the number of steps needed to eat each flower.
	We can see here a first advantage of knowledge modularity: a module can be used for multiple purposes.
	If we take the example of modules \emph{Move} and \emph{Eat}, an agent can use them to reason about its possibilities and the ones of others agents.
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{module_combination.pdf}
		\caption
		{
			\label{module_combination}
			Knowledge base of a rabbit divided into observations modules (dot ones) and theory modules (plain ones).
		}
	\end{figure}

	We have presented different ways to use agent knowledge via module combination.
	But these combinations are also a kind of knowledge and it can be known by the agent.
	For an agent, it is meta-knowledge on his knowledge, more precisely in this case it is knowledge about the use of his knowledge.
	This meta-knowledge is a part of the common theory of an agent and could be represented by theory modules.
	But to clarify the design of agent knowledge we dedicated a new kind of modules to represent it: meta-knowledge module.
	We define two kinds of these modules: combination module and decision module.
	
	\begin{definition}[Combination module]
		A combination module is a meta-knowledge module which represent an ASP modules combination.	
		The purpose of these modules is to represent simple behaviours and factorise meta-knowledge.
	\end{definition}
	
	A module combination can represent agent behaviour and by using meta-knowledge modules we can represent it in agent knowledge.
	The most simple behaviours can be represented by a simple list of modules to combine, like in examples \ref{hide_example}, 
	\ref{feed_example} and figures \ref{hide_figure}, \ref{feed_figure}.
	The module combination \{\emph{Wolf}, \emph{Myself}, \emph{Hiding}, \emph{Action}, \emph{Move}, \emph{Eat}, \emph{View}, \emph{Field}\} 
	and \{\emph{Myself}, \emph{Action}, \emph{Move}, \emph{Eat}, \emph{Field}, \emph{Flower}, \emph{Rabbit}\} 
	can respectively represent \emph{hide} and \emph{feed} behaviours of a rabbit.
	
	\begin{example}
		\label{hide_example}
		A meta-knowledge module which represent rabbit \emph{hide} behaviour:\newline
		\begin{module}{Run away}{6cm}
			\textbf{use\_module}("Wolf").\newline
			\textbf{use\_module}("Myself").\newline
			\textbf{use\_module}("Hiding").\newline
			\textbf{use\_module}("Action").\newline
			\textbf{use\_module}("Move").\newline
			\textbf{use\_module}("Eat").\newline
			\textbf{use\_module}("View").\newline
			\textbf{use\_module}("Field").
		\end{module}
	\end{example}
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{hide.pdf}
		\caption
		{
			\label{hide_figure}
			A meta-knowledge module (circle one) about \emph{hide} behaviour linked to modules it combines.
		}
	\end{figure}
	
	\begin{example}
		\label{feed_example}
		A meta-knowledge module which represent rabbit \emph{feed} behaviour:\newline
		\begin{module}{Feed}{6cm}
			\textbf{use\_module}("Myself").\newline
			\textbf{use\_module}("Action").\newline
			\textbf{use\_module}("Move").\newline
			\textbf{use\_module}("Eat").\newline
			\textbf{use\_module}("Field").\newline
			\textbf{use\_module}("Flower").\newline
			\textbf{use\_module}("Rabbit").
		\end{module}
	\end{example}
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{feed.pdf}
		\caption
		{
			\label{feed_figure}
			A meta-knowledge module about \emph{feed} behaviour linked to modules it combines.
		}
	\end{figure}
	
	\begin{definition}[Decision module]
		A decision module is a meta-knowledge module which defines the conditions to use ASP modules combinations.	
		The purpose of these modules is to represent dynamic behaviours by performing decision on how to reason and use knowledge.
	\end{definition}
	
	To represent more complex behaviours, meta-knowledge modules need knowledge on other meta-knowledge modules.
	Decision modules contain such knowledge, it can be used to represent dynamic behaviours and perform meta-reasoning.
	The \emph{survive} behaviour of a rabbit can be represent by such module, like in example \ref{survive_example} and figure \ref{survive_figure}.
	If a rabbit spotted a wolf it will use the \emph{Hunted} module. Otherwise it will use the \emph{Hunting} module.
	The \emph{survive} behaviour is dynamic: it depends on the current state of the world.
	
	\begin{example}
		\label{survive_example}
		A meta-knowledge module which describes the \emph{survive} behaviour of a rabbit:\newline
		\begin{module}{Survive}{6.5cm}
			predator $\leftarrow$ position(wolf,Position).\newline
			\textbf{use\_module}("Hunted") $\leftarrow$ predator.\newline
			\textbf{use\_module}("Hunting") $\leftarrow$ not predator.
		\end{module}
	\end{example}
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{survive.pdf}
		\caption
		{
			\label{survive_figure}
			A meta-knowledge module with knowledge about meta-knowledge module.
		}
	\end{figure}
	
	Multiple meta-knowledge modules can represent an agent's global behaviour.
	Figure \ref{behaviour_tree} represents rabbit's behaviour by a meta-knowledge modules tree.
	The behaviour of a rabbit is to survive, regarding the situation a rabbit is hunted by a wolf or not.
	When hunted, a rabbit has to run away if wolves are close or to search for a hiding place if not.
	If there is no predator, he will explore his environment to find flowers and when one is spotted, find a way to eat it.
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{behaviour_tree.pdf}
		\caption
		{
			\label{behaviour_tree}
			Representation of rabbit behaviour by a tree of meta-knowledge module.
			Decision module are represented by plain circle and combination modules by dot ones.
		}
	\end{figure}
	
\section{Framework}

	The previous section shows how ASP modules can represent agent common theory, observations and behaviours.
	To use this representation we define a reasoning architecture and a framework to use it in agent applications.
	As shown in figure \ref{framework_figure}, we divide agent reasoning into a cycle of three phases : acquisition-deduction-action.
	
	Our framework is different from the one of \cite{DBLP:journals/amai/KowalskiS99} which is observe-think-act.
	Here, the acquisition phase is not limited to the storage of new observations.
	During this phase, agent memory is updated to make it consistent with new observations.
	Old ones are modified and new ones are then stored in corresponding observations modules.
	If a rabbit sees a wolf at position P at time step T, he will store the information \emph{position(wolf,P)} in module \emph{Wolf}.
	Now at T+1 the rabbit sees position P but no wolf is at P, therefore the rabbit has to remove the fact \emph{position(rabbit,P)} from module \emph{Wolf}.
	The agent can also make direct deduction from observations: after a move, an agent will update his position regarding his previous one and the movement he performed.
	This phase is named acquisition, because the agent acquires observations by deducing their direct consequences on his memory.
	
	Having acquired observations, an agent will reason on what he can do regarding the current state of the world: it is the deduction phase.
	The purpose of this reasoning phase is to deduce what actions are possible to perform.
	Here we use an agent reasoning architecture based on ASP modules where meta-knowledge modules are used for reasoning.
	Decision modules use the agent's observations to define his behaviour and generate actions step by step.
	Knowledge produced by module combination is kept in next reasoning steps, 
	it means that answer set of the last reasoning step contains all deduction of previous ones.
	These last answer sets contain actions and their consequences on the environment and the agent.
	
	The figure \ref{modular_knowledge} represents rabbit's knowledge by an ASP modules graph.
	A node represents an ASP module and an edge represents the use of a module.
	Edges with plain arrows represent reasoning decisions and open ones represent module combination.
	In this representation, decision modules represent reasoning step.
	Here, meta-knowledge modules represent agent behaviour like in figure \ref{behaviour_tree}.
	In the example of figure \ref{modular_knowledge}, 
	the deduction phase will start by using \emph{Behaviour} module which specifies to combine \emph{Survive} module with wolves observations.
	By combining these modules the rabbit will decide if he is hunted or has to hunt.
	When he is hunted, he will use \emph{Hunted} module to decide if he has to run away or hide.
	Let's suppose that a wolf is too close to hide, then he will choose to run away, otherwise he will reason about where he can hide.
	In the case where there is no wolf, the rabbit will use \emph{Hunting} module with flowers observations.
	If there is no flower he will use \emph{Explore} module and when flowers are spotted he will use \emph{Feed} module.
	
	Module combination specifies by \emph{Run away} module generates all possible movements and the number of steps needed by each wolves to eat the rabbit after his move.
	The one of \emph{Hide} module add the theory module \emph{Hiding} and \emph{View} to this combination, to reason about where to hide regarding wolves sight range. 
	\emph{View} is also used with \emph{Discover} in \emph{Explore} module combination.
	Here, it is the knowledge about rabbits range of view that is used to reason about environment exploration.
	Finally, the module combination of \emph{Feed} module generates eating possibilities.
	It includes observations about other rabbits to compute if a flower can be eaten by another rabbit before the agent.
	
	Now the agent has to choose which actions to perform, it means choosing an answer set which allows him to approach or reach his goals.
	To make this choice we can use constraint and use some modules to compute scores to rank answer sets.
	Here, module \emph{Eat} computes the number of actions a predator has to perform to eat a prey.
	This module can be used by a rabbit to rank movement actions to go far away from a wolf or to approach and eat a flower.
	After choosing the best answer set, corresponding actions are performed and 
	the cycle continues with the acquisition of the real effect of these actions on the environment.

	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{framework.pdf}
		\caption
		{
			\label{framework_figure}
			Agent reasoning cycle.
		}
	\end{figure}

	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{modular_knowledge.pdf}
		\caption
		{
			\label{modular_knowledge}
			Representation of rabbit knowledge by an ASP modules graph.
		}
	\end{figure}
	
\subsection{Implementation}

	In this section we focus on the deduction phase of the framework and discuss about it implementation based on algorithm \ref{framework_algorithm}.
	The input of this algorithm is a set of modules and the output is a set of answer sets which contains actions that an agent can perform on its environment.
	The algorithm start by computing the combination of input modules and retrieves its answer sets by using an ASP solver, here we used \emph{clingo}.
	Answer sets are exploited in a deep first exploration by extracting keywords which define the combination of modules to use in the next reasoning step.
	When the answer set is totally parsed, we convert the current answer set into an ASP module and add it to the next combination of modules.
	Then, this combination is used for the next reasoning step and the cycle continues.
	Finally, when there is no more module to combine the algorithm returns a set of answer sets.
	The algorithm always finishes when an agent uses a module graph which is acyclic like in figure \ref{modular_knowledge}.

	Let's take again the example of the figure \ref{modular_knowledge} and let's suppose that the rabbit only acquires a set of new observations.
	First our agent will run the algorithm with the set of modules \{\emph{Behaviour}\} as input.
	This module will return one answer set which contains two literals: \emph{use\_module("Wolf")}, \emph{use\_module("Survive")}.
	Interpreting keywords \emph{use\_module}, the cycle continue by calling recursively the algorithm with \emph{Survive} module and wolf observations as input.
	A wolf is spotted, the answer set contains now \emph{use\_module("Hunted")}.
	Wolves observations are stored into a temporary module and combined with \emph{Hunted} module.
	A wolf is very near, the new answer set now contains \emph{use\_module("Run away")} and wolves observations.
	Wolves observations are stored again in a temporary module and combine with \emph{Run away} module.
	This combination specifies that the next one is \{\emph{Field}, \emph{Move}, \emph{Action}, \emph{Eat}, \emph{Myself}\}.
	Finally, the algorithm returns a set of answer sets which contains a movement action and its consequences.
	
	In this example, only a part of the knowledge is used for reasoning.
	Meta-knowledge modules \emph{Hide}, \emph{Hunting}, \emph{Explore} and \emph{Feed} are not used.
	Flowers and rabbits observations, like theory modules \emph{Hiding}, \emph{View} and \emph{Discover}, are ignored.
	The same knowledge can be represented by only one ASP program, but in this case, all knowledge is used for reasoning.
	Regarding this monolithic representation, modularity allows to reduce reasoning search space.

	\begin{algorithm}
	\caption{Combine}
	\label{framework_algorithm}
	\begin{algorithmic}[1]
	\STATE INPUT : <M> M a set of ASP modules
	\STATE OUTPUT : AS a set of answer set
	\newline
	\STATE AS a set of answer set
	\newline
	\STATE AS $\leftarrow$ \emph{clingo}(M)
	\newline
	\FOR {each answer set S of AS}
		\STATE M $\leftarrow \emptyset$ 
		\newline
		\STATE // Extract keywords
		\FOR {every literal L of S}
			\IF {L = "use\_module(Module)"}
				\STATE M $\leftarrow$ M $\cup$ Module
				\STATE S $\leftarrow$ S $/$ L
			\ENDIF
		\ENDFOR
		\newline
		\IF {M $\neq$ $\emptyset$}
			\STATE // Add S to M as a module
			\STATE M $\leftarrow$ M $\cup$ module(S)
			\STATE combine(M)
		\ENDIF
	\ENDFOR
	\newline
	\RETURN AS
	\end{algorithmic}
	\end{algorithm}

\section{Experiments}

	To evaluate our work, we implemented the algorithm \ref{framework_algorithm} and use it in a toy application based on the survival game example.
	In this application, the environment is a grid where each agent are located on a single square.
	Agents act turn by turn and have limited number of actions per turn.
	These actions can be: move to a square or eat an agent.
	To eat his prey, a predator has to be on the same square.
	These experimental results focus on the reasoning time of a single rabbit regarding a specific scenario.
	To evaluate our method we compare modular and monolithic reasoning time.
	The first one used algorithm \ref{framework_algorithm} and exploit meta-knowledge modules to reduce the quantity of knowledge to use.
	The second one directly runs the solver \emph{clingo} on the entire knowledge base: all theory and observations modules.
	In these experiments, the module combination \{\emph{Field}, \emph{Move}, \emph{Action}, \emph{Eat}, \emph{Myself}\} is always used at the end of reasoning.
	The difference between modular and monolithic representation is that,
	the first one can avoid the use of flowers observations and hiding possibility computation whereas the second one can not.
	
\subsection{Context}
	
	In these experiments, the rabbit's observations encompass the entire map, 
	this agent knows what is on each square: that includes the position of wolves, flowers and hiding place.
	The figure \ref{modular_knowledge_experiment} represents the modular knowledge of the rabbit.
	It is a simplified version of the one of figure \ref{modular_knowledge}, where rabbits have two behaviours: \emph{Hide} and \emph{Feed}.
	Here the rabbit have no fellows, there is no \emph{Rabbit} observations module.
	Monolithic representation always uses the entire knowledge base which correspond to the module combination
	\{\emph{Myself}, \emph{Wolf}, \emph{Flower}, \emph{Field}, \emph{Move}, \emph{Action} \emph{Eat}, \emph{Hiding}\}.
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{modular_knowledge_experiment.pdf}
		\caption
		{
			\label{modular_knowledge_experiment}
			Rabbit knowledge in experiments, simplified to behaviours \emph{hide} and \emph{feed}.
		}
	\end{figure}
	
	For these experiments, we use 4 different environments represented by figure \ref{environment_1}, \ref{environment_2} and \ref{environment_3}.
	The fourth environment is equivalent to the third one but ten times bigger regarding the number of squares, flowers and hiding places.
	Our evaluation is based on eight scenarios: for each environment, we evaluate the rabbit reasoning time regarding wolf presence.
	In scenarios where there is a wolf, reasoning about flowers is useless because hiding has a bigger priority.
	When there is no wolf, reasoning about hiding is no use because eating flowers is more important.
	In these experiments, we show that modular reasoning can be used to optimize reasoning by avoiding a part of agent knowledge which 
	is not necessary regarding the current situation.
	In all scenarios, monolithic reasoning considers movements regarding hiding and eating possibilities regardless the presence of a wolf.
	Modular reasoning considers movements to hide only when there is a wolf and considers eating only when there is no predator.
	
	The figure \ref{environment_1} represents the first scenario where there is no flower and no hiding place.
	In this scenario the rabbit just considers the movement he can perform without being caught by his predator at the next turn.
	This scenario is used to evaluate the time we lose when modularity reasoning does not reduce search space.
	The figure \ref{environment_2} represents the second scenario where there are 4 flowers and 4 hiding places.
	In this scenario, when there is no wolf the time lost by multiple steps of reasoning is almost compensated by the reduction of search space.
	The figure \ref{environment_3} represents the third scenario: agents are in a field of 25 flowers and 25 hiding places which are on the same square as flowers.
	The fourth scenario is a ten times bigger extension of the third one where the rabbit is in a field of 250 flowers and 250 hiding places. 
	In these two last scenarios, the number of flowers and hiding places is sufficient for modular representation to reduce reasoning time.
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{environment_1.png}
		\caption
		{
			\label{environment_1}
			Environment 1, a rabbit and a wolf without flowers and no place to hide.
		}
	\end{figure}

	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{environment_2.png}
		\caption
		{
			\label{environment_2}
			Environment 2, a rabbit, a wolf, 4 flowers and 4 hiding places (grass).
		}
	\end{figure}
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{environment_3.png}
		\caption
		{
			\label{environment_3}
			Environment 3, a rabbit and a wolf in a field of 25 flowers and 25 hiding places (same position as flower).
		}
	\end{figure}
	
\subsection{Results}

	\begin{table*}
		\label{experiment_results}
		\centering
		\begin{tabular}{ | c | c | c | c | c | c | c | c | }
		\hline
		Environment & Flower & Hiding place & Wolf & Representation & Time average & T-test & Runs\\	
		\hline
	 	\multirow{4}{*}{1} & \multirow{4}{*}{0} & \multirow{4}{*}{0} & \multirow{2}{*}{yes} & monolithic & 0.135 s & \multirow{2}{*}{0} & \multirow{16}{*}{1000} \\
		& & & & modular & 0.149 s & & \\ \cline{4-7}
		& & & \multirow{2}{*}{no} & monolithic & 0.107 s & \multirow{2}{*}{0} & \\
		& & & & modular & 0.119 s & & \\ \cline{1-7}
		\multirow{4}{*}{2} & \multirow{4}{*}{4} & \multirow{4}{*}{4} & \multirow{2}{*}{yes} & monolithic & 0.181 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{0.169 s} & & \\ \cline{4-7}
		& & & \multirow{2}{*}{no} & monolithic & 0.151 s & \multirow{2}{*}{$2.115E-5$} & \\
		& & & & modular & 0.152 s & & \\ \cline{1-7}
		\multirow{4}{*}{3} & \multirow{4}{*}{25} & \multirow{4}{*}{25} & \multirow{2}{*}{yes} & monolithic & 0.416 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{0.277 s} & & \\ \cline{4-7}
		& & & \multirow{2}{*}{no} & monolithic & 0.383 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{0.314 s} & & \\ \cline{1-7}
		\multirow{4}{*}{4} & \multirow{4}{*}{250} & \multirow{4}{*}{250} & \multirow{2}{*}{yes} & monolithic & 3.838 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{1.963 s} & & \\ \cline{4-7}
		& & &  \multirow{2}{*}{no} & monolithic & 3.819 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{2.739 s} & & \\ \cline{1-7}
		\hline
		\end{tabular}
		\caption
		{
			Experimental results of rabbit reasoning time on 8 scenarios of 4 different environments.
			For each method it shows the reasoning time average of 1000 runs and T-test result.
		}
	\end{table*}

	These experiments focus on rabbit reasoning time regarding modular and monolithic representation on 8 scenarios.
	This time corresponds to the run of algorithm \ref{framework_algorithm}, which includes \emph{clingo} solving and answer set parsing.
	The time we consider is the user one and to obtain a relevant value we compute the average running time on 1000 runs for each method on each scenario.
	These 16 sequences of 1000 runs tests have been performed consequently in the same running conditions on a Intel Core 2 Duo P8400 2.26GHz CPU.
	These experimental results are summarise in Table \ref{experiment_results}.
	
	In scenario 1, figure \ref{environment_1}, when there is a wolf, the monolithic reasoning run is about 0.135 seconds and the modular one is about 0.139 seconds.
	When there is no wolf, monolithic reasoning run is about 0.107 seconds and modular one is about 0.119 seconds.
	Here, the use of multiple reasoning steps by modular representation causes more time for reasoning because it does not reduce the search space.
	In scenario 2, figure \ref{environment_2}, in the case of the presence of a wolf, 
	run time of the monolithic reasoning up to 0.181 seconds because it now considers 4 flowers and 4 hiding places,
	and modular reasoning time just up to 0.169 seconds because it does not consider flowers.
	If there is no wolf, monolithic and modular reasoning time is almost the same.
	The time lost with reasoning about hiding in the monolithic representation is equivalent to the one lost by the multiple step reasoning.
	In this environment, modular reasoning is a little bit more efficient than monolithic one.
	For scenario 3, figure \ref{environment_3}, modular reasoning is more efficient in both cases.
	The proportion of flowers and hiding places is sufficient to make search space reduction interesting.
	When reasoning about hiding is the priority, modular reasoning takes 30\% less time than monolithic one and
	18\% less time when reasoning about eating.
	As the number of observations increases, the modular representation becomes more interesting: in scenario 4, when there is a wolf it takes 49\% less time to
	compute hiding actions and 32\% less time for reasoning about eating when there is no predator.
	
	These experiments show that modular knowledge representation and knowledge about modules combinations can be used to optimize reasoning by reducing search space.
	Here we show that this representation can be more efficient than monolithic one when entire knowledge base is not necessary to solve every situation.
	Designing such reasoning pattern implies to find the balance between the time lost by multiple reasoning steps and search space reduction.
	
\section{Conclusions and Outlook}

	We provide a method based on ASP modules to design agent knowledge and reasoning.
	This representation allows to intuitively implements dynamic behaviours via meta-reasoning.
	We also provide a framework which allows agent reasoning by module combination.
	Regarding an equivalent monolithic representation, a first improvement of our method is the reduction of reasoning search space.
	It also causes reduction of code size because modules are reusable for multiple purposes.
	
	In this paper, agent reasoning is very directed by meta-knowledge modules, 
	an interesting outlook will be to give the possibility to the agent to really choose which module combination he wants to use.
	Making agent able to build themselves a reasoning architecture like the one on figure \ref{modular_knowledge} is also an interesting research topic.
	Learning can also concern module content, an agent could choose to create new observations modules for storing specific information.
	Modular knowledge representation provides interesting perspectives for meta-reasoning and learning.
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{main}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%

%\nocite{*}

\end{document}

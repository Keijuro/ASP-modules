% This is "aamas2012 .tex" August 2012 
% This file should be compiled with "aamas2012 .cls" 
% This example file demonstrates the use of the 'aamas2012 .cls'
% LaTeX2e document class file. It is for those submitting
% articles to AAMAS 2012  conference. This file is based on
% the sig-alternate.tex example file.
% The 'sig-alternate.cls' file of ACM will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
% than the original style ACM style.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls ) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with AAMAS data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'aamas2012 .cls' you don't have control
% from within the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the IFAAMAS Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% These information will be overwritten by fixed AAMAS 2012  information
% in the style files - it is NOT as you are used with ACM style files.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%

\newtheorem{note}{Note}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

% This is the document class for full camera ready papers and extended abstracts repsectively 

\documentclass{aamas2012}

\usepackage{graphicx}
\usepackage{color}
\usepackage{algorithm,algorithmic}
\usepackage{multirow}
\usepackage{float}

\usepackage{module}

\graphicspath{{pictures/}}

% if you are using PDF LaTex and you cannot find a way for producing
% letter, the following explicit settings may help
 
\pdfpagewidth=8.5truein
\pdfpageheight=11truein

\begin{document}

	\begin{figure*}
		\includegraphics{page1.pdf}
	\end{figure*}
	
	\clearpage

\section{Introduction and motivation}

	In recent years, research about reasoning of multi-agent systems is very active.
	Some research works are concerned about knowledge representation like \cite{DBLP:conf/birthday/BaralG11},
	where authors discuss about representing actions in dynamic environment.
	There are also interesting works on meta-knowledge like \cite{DBLP:conf/atal/BaralGSP10}, 
	where agents have knowledge about other knowledge.
	In \cite{DBLP:conf/ijcai/SakamaSP11}, using this kind of meta-knowledge allows agents to lie to manipulate their fellows.
	Other works proposed agent knowledge representation like \cite{DBLP:conf/icmas/RaoG95}, 
	where agent knowledge is divide into beliefs, decisions and intentions (BDI).
	In \cite{DBLP:journals/amai/KowalskiS99} authors are concerned about representing agent reasoning via logic programming 
	and proposed an agent cycle based on three phases: observe, think, act.
	Regarding reasoning, there are works like \cite{DBLP:conf/datalog/Costantini10}, \cite{DBLP:conf/lpnmr/Costantini09}
	and \cite{DBLP:conf/aaaiss/BaralAD06} where authors are interested in modularity.
	
	In this work, our interest is about representing agent reasoning in dynamic environment.
	Our framework is based on modular knowledge representation.
	Agent knowledge is divided into different modules which are combined for reasoning.
	Using knowledge on modules combinations, an agent can adapt his reasoning regarding the situation.
	The purpose is to reduce reasoning search space by avoiding a part of agent knowledge.
	
	First, we consider the knowledge of an agent in two parts:
	\begin{itemize}
		\item $K$: non-revisable knowledge which consists of:
		\begin{itemize}
			\item $C$: the common theory
			\item $O$: the current observations
		\end{itemize}
		\item $B$: revisable knowledge which consists of:
		\begin{itemize}
			\item $M$: past observations 
		\end{itemize}
	\end{itemize}
		
	In such environment, an agent has to adapt to the evolution of the world to achieve his goals.
	Agent's knowledge has to be updated regarding world changes by adding new observations and updating old ones.
	
	Let $C_{T}, O_{T}, M_{T}$ be $C$, $O$ and $M$ at time step T and $\diamond$ a belief update operator.
	\begin{itemize}
		\item $C_{0} = C$, $O_{0} = \emptyset$, $M_{0} = \emptyset$
		\item $C_{T+1} = C$, $O_{T+1} = O$, $M_{T+1} = M_{T} \diamond O_{T+1}$
	\end{itemize}
	
	$T$ is the common background knowledge of all agent of a multi-agent system, it is considered as certain and not revisable.
	A common theory can also be limited to a group of agent which does not contain all the system.
	Observations are informations retrieved from the environment, it represents the current state of the world.
	If we consider that sensors are perfect then a current observation is not revisable.
	At time step 0, the knowledge of an agent is his common theory.
	This knowledge is extended by adding observations and its consequences at each new time step.

	$B$ is agent's beliefs, it represents informations assumed to be true by the agent and which are revisable.
	In a dynamic environment, if current observations are certain, it is not the case of past ones.
	Here we assumes that past observations are correct until new ones prove the contrary.
	At time step $T+1$, agent past observations $M_{T+1}$ is the knowledge base resulting from updating $M_{T}$ by $O_{T+1}$ using the knowledge update operator $\diamond$.

	For example, let's suppose that an agent $a$ sees another agent $b$ at position $p$ at time $T$.
	Now, at $T+1$, $a$ sees neither the position $p$ nor the agent $b$, then $a$ can assume that $b$ is always at position $p$ at $T+1$.
	When $a$ sees again the position $p$ or the agent $b$, he will update $M$ if needed.
	
	To make our work more understandable we will follow an intuitive example along our propositions: a survival game which represent a MAS in a dynamic environment.
	In this game there are three groups of agents: wolves, rabbits and flowers.
	Each kind of agent have specific goals and behaviours.
	To be simple, wolves eat rabbits and rabbits eat flowers.
	
	Wolves have only one goal: to feed.
	To reach this goal they have to catch and eat rabbits.
	A wolf can be in two situations: a prey is in sight or not.
	If there is no rabbit in the sight range of a wolf, the predator has to explore his environment to find one.
	When a prey is spotted, a wolf will try to perform a sneaky approach if it is not spotted himself, otherwise the predator will rush on his target.
	To summarise, a wolf have three behaviours: exploration, approach and attack.
	
	Rabbits have two goals : to feed and not to be eaten.
	On a first hand, a rabbit has to eat flowers and on an other hand it has to escape from wolf.
	Like a wolf, if no prey is in sight, a rabbit needs to explore its environment, but it can find preys and predators.
	If a wolf is spotted, surviving is more important than feeding, therefore a rabbit has to hide or run away.
	To sum up, a rabbit has four behaviours: explore, feed, hide and run away.
	In this paper, examples and experiments focus on rabbit agent regarding knowledge representation and reasoning.
	
	Flowers are passive agents: the environment has no effect on their behaviour.
	The only goal of a flower is to reproduce.
	A flower produces regularly a new one after a certain amount of time.
	Knowledge of a flower does not change regarding time.
	Flowers are independent, it does not care about other agents.

	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true,scale=3.0]{food_chain.png}
		\caption
		{
			\label{food_chain}
			A MAS in a dynamic environment where an agent is a wolf, a rabbit or a flower:
			Wolves eat rabbits and rabbits eat flowers.
		}
	\end{figure}

\section{Answer set programming}

	Answer set programming which is a specification and an implementation language, is very suitable to represent agent knowledge and reasoning.
	ASP is a form of declarative programming based on the stable model semantics of logic programming.
	In this section, we briefly introduce ASP paradigm and semantics.

	We recapitulate the basic elements of ASP in the following.
	An answer set program is a finite set of rules of the form
	$$a_{0}\ \emph{:-}\ a_{1},\ \ldots,\ a_{m},\ not\ a_{m+1},\ \ldots,\ not\ a_{n}\ (1)$$
	where $n \ge m \ge 0$, $a_{0}$ is a propositional atom or $\bot$, all
	$a_{1}, \ldots ,a_{n}$ are propositional atoms and the symbol "$not$" denotes default negation.
	If $a_{0} = \bot$, then Rule $(1)$ is a constraint (in which case $a_{0}$ is usually omitted).
	The intuitive reading of a rule of form $(1)$ is that whenever $a_{1}, \ldots, a_{m}$
	are known to be true and there is no evidence for any of the default negated atoms $a_{m+1}, \ldots, a_{n}$ to be true, then $a_{0}$ has to be true as well.
	Note that $\bot$ can never become true.

	In the ASP paradigm, the search of solution consist to compute answer sets of ASP program.
	An answer set for a program is defined following Gelfond and Lifschitz \cite{DBLP:conf/iclp/GelfondL88}.
	An interpretation $I$ is a finite set of propositional atoms.
	An atom a is true under $I$ if a $\in I$, and false otherwise.
	A rule $r$ of form (1) is true under $I$ if $\{a1,\ \dots,\ a_{m}\} \cup I$ and $\{a_{m+1},\ \ldots,\ a_{n}\} \cup I = \emptyset$ implies $a_{0} \in\ I$.
	Interpretation $I$ is a model of a program $P$ if each rule $r \in P$ is true under $I$.
	Finally, $I$ is an answer set of $P$ if $I$ is a subset-minimal model of $P^{I}$,
	where $P^{I}$ is defined as the program that results from $P$ by deleting all rules that contain a default negated atom from $I$, 
	and deleting all default negated atoms from the remaining rules.
	Programs can yield no answer set, one answer set, or many answer sets.
	To compute answer set of an ASP program, we run an ASP solver.
	
	\begin{example}
		\label{ASP_example}
		An ASP program composed of one fact, three rules and one constraint :\newline
		\newline
		rain.\newline
		stay :- not go\_out.\newline
		go\_out :- not stay.\newline
		wet :- rain, go\_out, not umbrella.\newline
		:- wet.
	\end{example}
	
	In example \ref{ASP_example}, the set of facts \{\emph{rain}, \emph{stay}\} is an answer set of the ASP program, but \{\emph{rain}, \emph{go\_out}, \emph{wet}\} is not, 
	because it contains \emph{wet} which is not consistent with the constraint \emph{:- wet}.

	Many research work use ASP to represent knowledge and reasoning such as \cite{DBLP:conf/atal/BaralGSP10} or \cite{DBLP:conf/clima/NieuwenborghVHV06}.
	In the first one, the authors use it to represent agent knowledge about other agents knowledge.
	In the second one, they focus on the flexibility of reasoning by introducing soft constraints: constraints which can be violate in some case.
	Other works like \cite{DBLP:conf/datalog/Costantini10} design ASP methods to improve some specific property of agent reasoning.
	In this work the author focuses on reactivity by using ASP modules where constraints are used as actions trigger.
	In \cite{DBLP:conf/lpnmr/Costantini09}, the author discuss about integration of ASP modules into agent reasoning.
	There are also works like \cite{DBLP:conf/aaaiss/BaralAD06} about modular ASP and 
	\cite{DBLP:conf/birthday/FaberW11} where they discuss about possible extensions of the paradigm.

\section{ASP modules}

	An ASP module is an ASP program which have a specific form and a specific use.
	The first advantage of these modules is their simplicity: a module is a little program which represents a specific knowledge.
	We can have a module which contains observations about surroundings,
	an other one to define what is a prey and a module dedicated to path computing.
	By combining these three modules, an agent can compute all paths to surrounding preys.
	To design agent knowledge we respect a module typology to represent background knowledge, observations and meta-knowledge.

\subsection{Typology}

	Following previous distinction of agent knowledge we define a typology to represent common theory $C$ and observations memory $M$,
	respectively by \emph{theory modules} and \emph{observations modules}.
	In the following figures, we represent \emph{theory modules} by plain rectangles and \emph{observations modules} by dot rectangles.

	\begin{definition}[Theory module]
		A theory module is a set of rules which represent knowledge about a specific domain.
		The set of all theory modules of an agent represent his background knowledge: the common theory $C$.
	\end{definition}
	
	\begin{example}
		\label{theory_example}
		
		A theory module of a rabbit about movement and one about actions where A is an agent, P a position, D and N are integer:\newline
		\begin{module}{Move}{8.4cm}
			can\_reach(A,P,D/N) :- distance(A,P,D), nb\_action(A,N).\newline
			can\_reach(A,P) :- can\_reach(A,P,0).
		\end{module}
		
		\begin{module}{eat}{8.4cm}
			food(me,flower).\newline
			food(wolf,me).\newline
			\newline
			can\_eat(A,Prey,D) :- food(A,Prey), position(Prey,P), distance(A,P,D).\newline
			can\_eat :- position(me,P), food(me,Prey), position(Prey,P).\newline
			\newline
			will\_eat(A,Prey,D) :- food(A,Prey), move(A,P), distance(Prey,P,D).\newline
			will\_eat(A,Prey,D) :- food(Predator,Prey), position(A,P1), move(Prey,P2), distance(P1,P2,D).\newline
		\end{module}
		
		\begin{module}{Action}{8.4cm}
			nb\_action(wolf,4).\newline
			nb\_action(rabbit,3).\newline
			\newline
			0\{action(move(P))\}1 :- can\_reach(me,P).\newline
			0\{action(eat)\}1 :- can\_eat.\newline
			0\{action(hide)\}1 :- can\_hide.\newline
			:- action(A1), action(A2), A1 != A2.\newline
			\newline
			move :- action(move(P)).\newline
			wolf\_range :- position(me,P), can\_reach(wolf,P).\newline
			wolf\_close :- position(me,P), distance(wolf,P,D), D <= 2.\newline
			\newline
			danger\_after(S) :- move(me,P), can\_reach(wolf,P,S).\newline
			danger :- wolf\_range, not move, not hide.\newline
			danger :- action(hide), wolf\_close.\newline
			danger :- danger\_after(0).\newline
			safe :- not danger.
		\end{module}
		
	\end{example}
	
	Example \ref{theory_example} shows three theory modules, the first one represents knowledge about movement and the second one represents knowledge about actions.
	To simplify our examples, \emph{distance/3} is supposed given and his third argument is the distance between an agent $A$ and a position $P$ or two positions $P1$, $P2$.
	Predicate \emph{nb\_action/2} specifies the number of actions that an agent can perform in one time step.
	The first module can be used by a rabbit to compute his movement possibility and the ones of his predators.
	It specify the number of step an agent needs to reach a position $P$.
	The second module specify the number of actions an agent needs currently to eat a prey.
	It also consider it regarding predator and prey movement.
	The third module defines the three actions that a rabbit can perform: move, eat and hide.
	It specifies that only one action can be performed at once.
	This module contain knowledge about consequence of actions, here \emph{danger/0} represent the possibility to be eaten by a wolf at next time step.
	Predicate \emph{danger\_after/1} represent the number of time step needed to be in danger.
	Let suppose that we have a fourth module similar to \emph{eat} one but regarding hiding possibilities.
	By combining these four modules with his observations a rabbit will produce all actions he can perform and their consequences regarding safety, eating and hiding.

	\begin{definition}[Observations module]
		An observations module is a set of facts which represents related observations.
		The content of such module changes regarding time.
		The set of all observations modules of an agent represents his past observations $M$.
	\end{definition}
	
	\begin{example}
		\label{observations_example}
		An observations module of a rabbit about wolves positions and one about himself :\newline
		\begin{module}{Wolf}{6cm}
			position(wolf,coord(0,3)).\newline
			position(wolf,coord(-1,2)).\newline
			position(wolf,coord(2,-4)).
		\end{module}
		
		\begin{module}{Myself}{6cm}
			me("Bugs Bunny").\newline
			position(me,coord(0,2)).\newline
			nb\_action(me,3).
			hidden.
		\end{module}
	\end{example}

	Example \ref{observations_example} shows two \emph{observations modules}, 
	the first one regroup wolves positions and the second one contains agent personal observations.
	Using \emph{theory} and \emph{observations} modules we can represent agent background knowledge $C$ and past observations $M$.
	The figure \ref{module_combination} represents the knowledge of a rabbit by a set of observations modules and theory modules.
	Regarding how we combined these modules with \emph{theory} ones, we can produce different kinds of knowledge.
	Here, by combining modules \emph{Myself}, \emph{Field} and \emph{Move} a rabbit will produce all his possibility of movements.
	The combination \{\emph{Rabbit}, \emph{Field}, \emph{Move}\} will produce all possible movements that other rabbits can perform. 
	By combining \{\emph{Myself}, \emph{Field}, \emph{Flower}, \emph{Move}, \emph{Eat}, \emph{Action}\} 
	a rabbit will produce all possible actions to feed he can perform now and their influence on the number of steps needed to eat each flower.
	We can see here a first advantage of knowledge modularity: a module can be used for multiple purposes. 
	For example, module \emph{Move} can be used to reason about both agent's, fellows' and predators' movements.

	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{module_combination.pdf}
		\caption
		{
			\label{module_combination}
			Knowledge base of a rabbit divided into observations modules (dot ones) and theory modules (plain ones).
		}
	\end{figure}

	We have presented different ways to use agent knowledge via module combination.
	But these combinations are also a kind of knowledge and it can be very useful to an agent to know it.
	For an agent, it is meta-knowledge on his knowledge, more precisely in this case it is knowledge about the use of his knowledge.
	This meta-knowledge is a part of the common theory of an agent and could be represented by theory modules.
	But to clarify the design of agent knowledge we dedicated a new kind of modules to represent it: \emph{meta-knowledge modules}.
	We define two kinds of these modules: \emph{combination modules} and \emph{decision modules}.
	The first ones represent a unique module combination.
	The second ones use theory and observations to make decision on which module combination to use for reasoning.
	To distinguish \emph{decision modules} and \emph{combination modules}, we represent them respectively by plain and dot circles.
	Modules represented by dot form only contain facts and plain ones can contain rules and constraints.
	
	\begin{definition}[Combination module]
		A combination module is a meta-knowledge module which represent a unique ASP modules combination.
		This combination can include theory, observations and combination modules.
	\end{definition}
	
	\begin{example}
		\label{movement_example}
		A combination module which represent the module combination to reason about movements:
		\begin{module}{Movements}{6.5cm}
			\#include("Move").\newline
			\#include("Field").
		\end{module}
	\end{example}
	
	\begin{figure*}
		\centering
		\includegraphics[keepaspectratio=true, scale=1]{meta-knowledge.pdf}
		\caption
		{
			\label{meta-knowledge}
			Rabbit meta-knowledge representation by \emph{combination modules} (dot circle).
			Arrows represent module inclusion: keyword \emph{include}(Module).
		}
	\end{figure*}
	
	Meta-knowledge about modules combinations can be used to divide agent reasoning into multiple part to clarify its representation,
	like in figure \ref{meta-knowledge}.
	Furthermore, meta-knowledge can be used to optimize reasoning by reducing the use of knowledge.
	Using \emph{decision modules} we can exploit this representation to control agent reasoning and optimize it.
	
	\begin{definition}[Decision module]
		A decision module is a meta-knowledge module which defines the conditions to use ASP modules combinations.
		A graph of such module can represent agent reasoning and behaviours.
	\end{definition}
	
	\begin{example}
		\label{survive_example}
		A decision module which describes the \emph{survive} behaviour of a rabbit:\newline
		\begin{module}{Survive}{6.5cm}
			\#include("Wolf").\newline
			\newline
			predator :- position(wolf,Position).\newline
			\textbf{next}("Hunted") :- predator.\newline
			\textbf{next}("Hunter") :- not predator.
		\end{module}
	\end{example}
	
	\begin{figure*}
		\centering
		\includegraphics[keepaspectratio=true, scale=1]{reasoning.pdf}
		\caption
		{
			\label{reasoning}
			Representation of rabbit reasoning by \emph{decision modules} (plain circle).
			Plain arrows represent the choice of next reasoning step: keyword \emph{next}(Module).
		}
	\end{figure*}
	
	\emph{Decision} modules are used to make decision on what it is useful to reason about regarding the situation.
	Figure \ref{reasoning} shows the introduction of \emph{decision} modules into knowledge representation of figure \ref{meta-knowledge}.
	Here we replace \emph{My actions} module by a \emph{decision modules} graph based on rabbit behaviours.
	Example \ref{survive_example}, show the content of \emph{Survive} module which represent the main behaviour of a rabbit.
	A rabbit combine this \emph{decision module} with observations about wolves to decide if it is hunted or not.
	
	\begin{example}
		\label{hunted_example}
		A decision module which describes the \emph{hunted} behaviour of a rabbit:\newline
		\begin{module}{Hunted}{6.5cm}
			\#include("Wolf").\newline
			\#include("Myself").\newline
			\newline
			hide :- hidden, position(me,P), distance(wolf,P,D), D <= 2.\newline
			\textbf{next}("Hide") :- hide.\newline
			\textbf{next}("Run away") :- not hide.
		\end{module}
	\end{example}
	
	If a predator is present, a rabbit will use \emph{Hunted} module, shows in example \ref{hunted_example},
	to choose regarding wolves distance, if it has to stay hidden or run away to find a place to hide.
	
	\begin{example}
		\label{hunter_example}
		A decision module which describes the \emph{hunter} behaviour of a rabbit:\newline
		\begin{module}{Hunter}{6.5cm}
			\#include("Flower").\newline
			\newline
			prey :- position(flower,P).\newline
			\textbf{next}("Feed") :- prey.\newline
			\textbf{next}("Explore") :- not prey.
		\end{module}
	\end{example}
	
	If there is no predator, a rabbit will use \emph{Hunter} module, shows in example \ref{hunter_example}.
	Regarding flower presence, it will reason about how to eat one or find one.
	These \emph{decision modules} are used to reduce the quantity of knowledge used for reasoning.
	In these example, when a rabbit reason about how to run away, it does not reason about eating or exploration and vice versa.
	Using modular representation we can avoid a part of agent knowledge to focus reasoning on current priority.
	
\section{Framework}

	The previous section shows how ASP modules can represent agent common theory, observations, meta-knowledge and reasoning.
	To use this representation we define a reasoning architecture and a framework to use it in agent applications.
	As shown in figure \ref{framework_figure}, we divide agent reasoning into a cycle of three phases : \emph{acquisition-analyse-action}.
	Our framework is similar from the one of \cite{DBLP:journals/amai/KowalskiS99} which is \emph{observe-think-act}.
	One difference here, is that the acquisition phase is not limited to the storage of new observations.
	
	Acquisition phase: during this phase, agent past observation $M$ is updated to make it consistent with new observations $O$.
	Old ones are modified or deleted and new ones are then stored in corresponding observations modules.
	For example, if a rabbit sees a wolf at position P at time step T, the information \emph{position(wolf,P)} is then stored in module \emph{Wolf}.
	Now at T+1 the rabbit sees position P but no wolf is at P, therefore the rabbit has to remove the fact \emph{position(wolf,P)} from module \emph{Wolf}.
	An agent can also make direct deduction from observations: after a move, an agent will update his position regarding his previous one and the movement he performed.
	This phase is named acquisition, because the agent acquires observations by deducing their direct consequences on his past observations.
	In this framework, a part of agent common theory $C$ is dedicated to updating past observations $M$.
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.4]{framework.pdf}
		\caption
		{
			\label{framework_figure}
			Agent reasoning cycle.
		}
	\end{figure}
	
	Analyse phase: having acquired observations, an agent will reason on what he can do regarding the current state of the world.
	The purpose of this reasoning phase is to deduce what actions are possible to perform and their consequences.
	Here we use an agent reasoning architecture based on ASP modules where meta-knowledge modules are used for reasoning like in figure \ref{reasoning}.
	
	Here rabbit's reasoning is represented by an ASP modules graph.
	A node represents an ASP module and arrows represents module combinations.
	Edges with plain arrows represent reasoning decisions and empty ones represent module inclusion.
	In this representation, \emph{decision modules} represent reasoning steps.
	In this example, the analyse phase will start by using \emph{Survive} module with wolves observations.
	By combining these modules the rabbit will decide if he is hunted or has to hunt.
	When he is hunted, he will use \emph{Hunted} module to decide if he has to run away or hide.
	Let's suppose that a wolf is too close to hide, then he will choose to run away, otherwise he will reason about where he can hide.
	In the case where there is no wolf, the rabbit will use \emph{Hunter} module with flowers observations.
	If there is no flower he will use \emph{Explore} module and when flowers are spotted he will use \emph{Feed} module.
	
	Action phase: now the agent has to choose which actions to perform, it means choosing an answer set which allows him to approach or reach his goals.
	To make this choice we can use constraints and use some modules to compute scores to rank answer sets.
	Here, module \emph{Eat} computes the number of actions a predator has to perform to eat a prey.
	It can be used by a rabbit to rank movement actions to go far away from a wolf or to approach and eat a flower.
	After choosing the best answer set, corresponding actions are performed and 
	the cycle continues with the acquisition of the real effect of these actions on the environment.

	Let $n$ be a reasoning step, regarding \emph{observe-think-act} framework, ours can be sum up as:
	\begin{itemize}
		\item Acquisition phase: $observe, think, update$.
		\item Analyse phase: $think_{0}, decide_{0}, \ldots, decide_{n}, think_{n}$.
		\item Action phase: $decide, act$.
	\end{itemize}
\subsection{Implementation}

	In this section we focus on the deduction phase of the framework and discuss about it implementation based on algorithm \ref{framework_algorithm}.
	The input of this algorithm is a set of modules and the output is a set of answer sets which contains actions that an agent can perform on its environment.
	The algorithm start by computing the combination of input modules and retrieves its answer sets by using an ASP solver.
	Answer sets are exploited in a deep first exploration by extracting keywords "\emph{next}" which define the combination of modules to use in the next reasoning step.
	The cycle continue until there is no more module to combine, then the algorithm returns a set of answer sets.
	The algorithm always finishes when an agent uses a module graph which is acyclic like in figure \ref{reasoning}.

	Let's take again the example of the figure \ref{reasoning} and let's suppose that the rabbit just finish to acquires new observations.
	This agent will start his analyse phase by running the algorithm with the set of modules \{\emph{Survive}, \emph{Wolf}\}.
	This combination will return one answer set which contains wolfs observations and two literals: \emph{predator} and \emph{next("Hunted")}.
	Interpreting keywords \emph{next}, the cycle continue by calling recursively the algorithm with \{\emph{Hunted}, \emph{wolf}, \emph{Myself}\} as input, 
	because \emph{Hunted} module specify to include \emph{wolf} and \emph{Myself}.
	This new combination produce one answer set which contains observations of module \emph{Wolf} and \emph{Myself} and one literals: \emph{next("Run away")}.
	After extracting "\emph{include}" keyword from \emph{Run away} and from all \emph{combination module} it include, 
	the last combination can be sum up by \{\emph{Hiding}, \emph{Move}, \emph{Myself}, \emph{Field}, \emph{Wolf}, \emph{Action}\}.
	Finally, the algorithm returns a set of answer sets which contains a movement action and its consequences regarding safety and hiding possibilities.
	
	In this example, only a part of agent knowledge is used for reasoning.
	\emph{Meta-knowledge modules} \emph{Hide}, \emph{Hunter}, \emph{Explore}, \emph{Feed}, discovery ones and food ones are not used.
	Flowers and rabbits observations, as theory modules \emph{Eat}, \emph{View} and \emph{Discover}, are ignored.
	The same knowledge can be represented by only one ASP program, but in this case, all knowledge is used for reasoning.
	Regarding this monolithic representation, modularity allows to reduce reasoning search space.

	\begin{algorithm}
	\caption{Combine}
	\label{framework_algorithm}
	\begin{algorithmic}[1]
	\STATE INPUT : <M> M a set of ASP modules
	\STATE OUTPUT : AS a set of answer set
	\newline
	\STATE AS a set of answer set
	\newline
	\STATE AS $\leftarrow$ \emph{ASPsolver}(M)
	\newline
	\FOR {each answer set S of AS}
		\STATE M $\leftarrow \emptyset$ 
		\newline
		\STATE // Extract keywords
		\FOR {every literal L of S}
			\IF {L = "\emph{next}(Module)"}
				\STATE M $\leftarrow$ M $\cup$ \{Module\}
			\ENDIF
		\ENDFOR
		\newline
		\IF {M $\neq$ $\emptyset$}
			\RETURN combine(M)
		\ENDIF
	\ENDFOR
	\newline
	\RETURN AS
	\end{algorithmic}
	\end{algorithm}

\section{Experiments}

	To evaluate our work, we implemented the algorithm \ref{framework_algorithm} in a toy application based on the survival game example.
	In this application, the environment is a grid where each agent are located on a single square.
	Agents act turn by turn and have limited number of actions per turn.
	These actions can be: move to a square, eat an agent or hide.
	To eat his prey, a predator has to be on the same square, it is the same rule for hiding.
	These experimental results focus on the reasoning time of a single rabbit regarding a specific scenario.
	To evaluate our method we compare modular and monolithic reasoning time.
	The first one used algorithm \ref{framework_algorithm} and exploit \emph{meta-knowledge modules} to reduce the quantity of knowledge to use for reasoning.
	The second one directly runs the solver \emph{clingo} on the entire knowledge base: all theory and observations modules.
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.45]{application.png}
		\caption
		{
			\label{application}
			Experimental application based on the survival game example.
		}
	\end{figure}
	
\subsection{Context}
	
	In these experiments, the rabbit's observations encompass the entire map, 
	this agent knows what is on each square: that includes the position of wolves, flowers and hiding place.
	Here modular knowledge of the rabbit is the one of figure \ref{reasoning} and modules are almost the same as the ones presented in previous example.
	Monolithic representation always uses the entire knowledge base which correspond to the module combination
	\{\emph{Myself}, \emph{Wolf}, \emph{Rabbit}, \emph{Flower},  \emph{Field}, \emph{View}, \emph{Eat}, \emph{Discover}, \emph{Move}, \emph{Action}, \emph{Hiding}\}.

	For these experiments, we use 4 different environments represented by figure \ref{environment_1}, \ref{environment_2} and \ref{environment_3}.
	The fourth environment is equivalent to the third one but ten times bigger regarding the number of squares, flowers and hiding places.
	Our evaluation is based on eight scenarios: for each environment, we evaluate the rabbit reasoning time regarding wolf presence.
	In scenarios where there is a wolf, reasoning about flowers or exploration is useless because hiding has a bigger priority.
	When there is no wolf, reasoning about hiding or exploration is no use because eating flowers is more important.
	In these experiments, we show that modular reasoning can be used to optimize reasoning by avoiding a part of agent knowledge which 
	is not necessary regarding the current situation.
	In all scenarios, monolithic reasoning considers movements regarding hiding and eating possibilities regardless the presence of a wolf.
	Modular reasoning considers movements to hide only when there is a wolf and considers eating only when there is no predator.
	It only consider exploration in the first scenario where there is no flower and when there is no wolf.
	
	The figure \ref{environment_1} represents the first scenario where there is no flower and no hiding place.
	In this scenario the rabbit only considers the movement he can perform without being caught by his predator at the next turn.
	This scenario is used to evaluate the time we lose when modularity reasoning does not reduce search space.
	The figure \ref{environment_2} represents the second scenario where there are 4 flowers and 4 hiding places.
	In this scenario, when there is no wolf the time lost by multiple steps of reasoning is almost compensated by the reduction of search space.
	The figure \ref{environment_3} represents the third scenario: agents are in a field of 25 flowers and 25 hiding places which are on the same square as flowers.
	The fourth scenario is a ten times bigger extension of the third one where the rabbit is in a field of 250 flowers and 250 hiding places. 
	In these two last scenarios, the number of flowers and hiding places is sufficient for modular representation to reduce reasoning time.
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{environment_1.png}
		\caption
		{
			\label{environment_1}
			Environment 1, a rabbit and a wolf without flowers and no place to hide.
		}
	\end{figure}

	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{environment_2.png}
		\caption
		{
			\label{environment_2}
			Environment 2, a rabbit, a wolf, 4 flowers and 4 hiding places (grass).
		}
	\end{figure}
	
	\begin{figure}
		\centering
		\includegraphics[keepaspectratio=true, scale=0.5]{environment_3.png}
		\caption
		{
			\label{environment_3}
			Environment 3, a rabbit and a wolf in a field of 25 flowers and 25 hiding places (same position as flower).
		}
	\end{figure}
	
\subsection{Results}

	%TODO

	\begin{table*}
		\label{experiment_results}
		\centering
		\begin{tabular}{ | c | c | c | c | c | c | c | c | }
		\hline
		Environment & Flower & Hiding place & Wolf & Representation & Time average & T-test & Runs\\	
		\hline
	 	\multirow{4}{*}{1} & \multirow{4}{*}{0} & \multirow{4}{*}{0} & \multirow{2}{*}{yes} & monolithic & 0.135 s & \multirow{2}{*}{0} & \multirow{16}{*}{1000} \\
		& & & & modular & 0.149 s & & \\ \cline{4-7}
		& & & \multirow{2}{*}{no} & monolithic & 0.107 s & \multirow{2}{*}{0} & \\
		& & & & modular & 0.119 s & & \\ \cline{1-7}
		\multirow{4}{*}{2} & \multirow{4}{*}{4} & \multirow{4}{*}{4} & \multirow{2}{*}{yes} & monolithic & 0.181 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{0.169 s} & & \\ \cline{4-7}
		& & & \multirow{2}{*}{no} & monolithic & 0.151 s & \multirow{2}{*}{$2.115E-5$} & \\
		& & & & modular & 0.152 s & & \\ \cline{1-7}
		\multirow{4}{*}{3} & \multirow{4}{*}{25} & \multirow{4}{*}{25} & \multirow{2}{*}{yes} & monolithic & 0.416 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{0.277 s} & & \\ \cline{4-7}
		& & & \multirow{2}{*}{no} & monolithic & 0.383 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{0.314 s} & & \\ \cline{1-7}
		\multirow{4}{*}{4} & \multirow{4}{*}{250} & \multirow{4}{*}{250} & \multirow{2}{*}{yes} & monolithic & 3.838 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{1.963 s} & & \\ \cline{4-7}
		& & &  \multirow{2}{*}{no} & monolithic & 3.819 s & \multirow{2}{*}{0} & \\
		& & & & modular & \textbf{2.739 s} & & \\ \cline{1-7}
		\hline
		\end{tabular}
		\caption
		{
			Experimental results of rabbit reasoning time on 8 scenarios of 4 different environments.
			For each method it shows the reasoning time average of 1000 runs and T-test result.
		}
	\end{table*}

	These experiments focus on rabbit reasoning time regarding modular and monolithic representation on 8 scenarios.
	This time corresponds to the run of algorithm \ref{framework_algorithm}, which includes \emph{clingo} solving and answer set parsing.
	The time we consider is the user one and to obtain a relevant value we compute the average running time on 1000 runs for each method on each scenario.
	These 16 sequences of 1000 runs tests have been performed consequently in the same running conditions on a Intel Core 2 Duo P8400 2.26GHz CPU.
	These experimental results are summarise in Table \ref{experiment_results}.
	
	In scenario 1, figure \ref{environment_1}, when there is a wolf, the monolithic reasoning run is about 0.135 seconds and the modular one is about 0.139 seconds.
	When there is no wolf, monolithic reasoning run is about 0.107 seconds and modular one is about 0.119 seconds.
	Here, the use of multiple reasoning steps by modular representation causes more time for reasoning because it does not reduce the search space.
	In scenario 2, figure \ref{environment_2}, in the case of the presence of a wolf, 
	run time of the monolithic reasoning up to 0.181 seconds because it now considers 4 flowers and 4 hiding places,
	and modular reasoning time just up to 0.169 seconds because it does not consider flowers.
	If there is no wolf, monolithic and modular reasoning time is almost the same.
	The time lost with reasoning about hiding in the monolithic representation is equivalent to the one lost by the multiple step reasoning.
	In this environment, modular reasoning is a little bit more efficient than monolithic one.
	For scenario 3, figure \ref{environment_3}, modular reasoning is more efficient in both cases.
	The proportion of flowers and hiding places is sufficient to make search space reduction interesting.
	When reasoning about hiding is the priority, modular reasoning takes 30\% less time than monolithic one and
	18\% less time when reasoning about eating.
	As the number of observations increases, the modular representation becomes more interesting: in scenario 4, when there is a wolf it takes 49\% less time to
	compute hiding actions and 32\% less time for reasoning about eating when there is no predator.
	
	These experiments show that modular knowledge representation and knowledge about modules combinations can be used to optimize reasoning by reducing search space.
	Here we show that this representation can be more efficient than monolithic one when entire knowledge base is not necessary to solve every situation.
	Designing such reasoning pattern implies to find the balance between the time lost by multiple reasoning steps and search space reduction.
	
\section{Conclusions and Outlook}

	We provide a method based on ASP modules to design agent knowledge and reasoning.
	This representation allows to intuitively implements dynamic behaviours via meta-reasoning.
	We also provide a framework which allows agent reasoning by module combination.
	Regarding an equivalent monolithic representation, a first improvement of our method is the reduction of reasoning search space.
	It also causes reduction of code size because modules are reusable for multiple purposes.
	
	In this paper, agent reasoning is very directed by meta-knowledge modules, 
	an interesting outlook will be to give the possibility to the agent to really choose which module combination he wants to use.
	Making agent able to build themselves a reasoning architecture like the one on figure \ref{modular_knowledge} is also an interesting research topic.
	Learning can also concern module content, an agent could choose to create new observations modules for storing specific information.
	Modular knowledge representation provides interesting perspectives for meta-reasoning and learning.
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{apalike}
\bibliography{main}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%

%\nocite{*}

\end{document}
